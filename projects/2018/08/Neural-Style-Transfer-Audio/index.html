<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Neural Style Transfer on Audio Signals | Alish Dipani </title> <meta name="author" content="Alish Dipani"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="neuro-ai, computational-cognitive-neuroscience, vision-science"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://alishdipani.github.io/projects/2018/08/Neural-Style-Transfer-Audio/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Alish</span> Dipani </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/journals/">Journals </a> </li> <li class="nav-item "> <a class="nav-link" href="/conferences/">Conferences </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/awards/">Awards </a> </li> <li class="nav-item "> <a class="nav-link" href="/service/">Service </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">More </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/invited-talks/">Invited Talks</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/experience/">Experience</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/open-source/">Open-source</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Neural Style Transfer on Audio Signals</h1> <p class="post-meta"> Created in August 29, 2018 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2018   ·   <i class="fa-solid fa-hashtag fa-sm"></i> style-transfer,   <i class="fa-solid fa-hashtag fa-sm"></i> audio-processing </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Summary: Generation of new music using Neural Style Transfer Algorithm.</p> <h1 id="introduction"><ins>Introduction</ins></h1> <p>“Style Transfer” on images has recently become very popular and an active research topic, this shows how Convolutional Neural Networks(CNNs) have the power to adapt to a great variety of tasks. Here, we extend and modify this algorithm for audio signals and use the power of CNNs and generate new audio from a style audio that can be the tune or the beat and a content audio that can be someone just speaking the lyrics of a song.</p> <h1 id="neural-style-transfer-on-images"><ins>Neural Style Transfer on Images</ins></h1> <p>“Neural Style Transfer” was originally for images, the idea is to use a CNN model for extracting the style of an image called style image and content of another image called content image and generating a new image having the style of the style image and content of the content image. This is done by encoding the two images using a CNN Model and then taking a white noise image and minimizing the loss between this image and content and style images so that it has the content same as the content image and style as style image. <br><br> Let \(\vec{p}\) be the content image, \(\vec{a}\) be the style image and \(\vec{x}\) be the white noise image (i.e. the generated image) which will be the final image.<br> So, total loss is sum of content loss and style loss.<br> \(L_{total}(\vec{p},\vec{a},\vec{x}) = \alpha L_{content}(\vec{p},\vec{x}) + \beta L_{style}(\vec{a},\vec{x})\)</p> <h3 id="cnn-model">CNN Model</h3> <p>A deep CNN model is chosen to extract the features of images. Deep CNN models provide proper encoding for the features of images. A Model like VGG-19 is chosen having a large number of convolutional layers. Pre-trained model is used as they provide proper encoding.</p> <p><img src="https://www.pyimagesearch.com/wp-content/uploads/2018/08/neural_style_transfer_gatys.jpg" alt="alt text"></p> <h3 id="content-loss">Content Loss</h3> <p>The content loss is the Mean squared error between the encoding of the white noise image and the content image.<br> For a layer \(l\) and the input image \(\vec{x}\), let the number of filters be \(N_{l}\) and so the output(or encoded) image will have \(N_{l}\) feature maps, each of size \(M_{l}\), where \(M_{l}\) is the height times width. So, the encoded image of layer can be stored in a matrix \(F_{l} \epsilon R^{ N_{l}xM_{l} }\). Where \(F^{l}_{ij}\) is the activation of \(i^{th}\) filter at position \(j\) in layer \(l\).<br> \(L_{content}(\vec{p},\vec{x},l) = \frac{1}{2} \sum_{i,j}(F^{l}_{ij} - P^{l}_{ij})^{2}\)</p> <h3 id="style-loss">Style Loss</h3> <p>For capturing the style a style representation is used which computer the correlations between the different filter responses, where the expectation is taken over the spatial extent of the input image. These feature correlations are given by Gram Matrix \(G^{l} \epsilon R^{ N_{l}xN_{l} }\), where \(G^{l}_{ij}\) is the inner product between the feature maps \(i\) and \(j\) represented by vectors in layer \(l\) and \(N_{l}\) is the number of feature maps.<br> \(G^{l}_{ij}= \sum_{k}F^{l}_{ik}F^{l}_{jk}\).<br> And so the Style loss is the Mean squared error between the gram matrices of Style image and the white noise image.<br> Let \(\vec{a}\) be the style image and \(\vec{x}\) be the white noise image. Let \(A^{l}\) and \(X^{l}\) be the style representations of of style image and white noise image in layer \(l\). So, Total style loss of a layer \(l\) is \(E_{l}\).<br> \(E_{l} = \frac{\sum_{i,j}(X^{l}_{ij}-A^{l}_{ij})^{2}}{4N^{2}_{l}M^{2}_{l}}\)<br> So, the total style is<br> \(L_{style}(\vec{a},\vec{x}) = \sum^{L}_{l=0}w_{l}E_{l}\)<br> where \(w_{l}\) are the weighting factor of each layer.</p> <h3 id="hyperparameter-tuning">Hyperparameter tuning</h3> <p>To calculate the style and content loss, standard error back-propagation is done. To calculate \(L_{total}\), \(L_{content}\) is weighted by \(\alpha\) and \(L_{style}\) is weighted by \(\beta\).<br> The ratio of \(\frac{\alpha}{\beta}\) is generally kept \(10^{-3}\) or \(10^{-4}\), this prevents the style from dominating and therefore preventing the loss of content.</p> <h1 id="neural-style-transfer-on-audio-signals"><ins>Neural Style Transfer on Audio Signals</ins></h1> <p>The base idea for Neural Style algorithm for audio signals is same as for images, the extracted style of the style audio is to be applied to the generated audio. Here, the content audio is directly used for generation instead of noise audio as this prevents calculation of content loss and eliminates the noise from the generated audio.</p> <h3 id="model-selection">Model Selection</h3> <p>For Audio signals 1 Dimensional Convolutions are used as they have different spatial features than images. So, models having 1-D CNN layers is used.<br> It is observed that shallow models perform better than deep models and so a shallow model having only one layer but having large number of filters is used. The models are not pre-trained and have random weights as it is observed that it does not make a difference as we only need the encoding.</p> <h3 id="pre-processing">Pre-processing</h3> <p>An audio signal has to be converted to frequency domain from time domain because the frequencies have the spatial features of audio signals. The raw audio is converted to spectrogram via Short Time Fourier Transform(STFT). Spectrogram is a 2D Representation of a 1D signal, Spectrogram has \(C\) channels and \(S\) samples for every channel. So, a spectrogram can be considered as an \(1xS\) image with \(C\) channels.</p> <p><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/7c592e7e00422dc7a76ead5932e34eafb5bef704/2-Figure2-1.png" alt="alt text"></p> <h3 id="content-loss-1">Content Loss</h3> <p>Here, as the content audio is used for generation of the new audio i.e. the generated audio, content loss is not taken into consideration. However, it can be taken into consideration.<br> So, here total loss is just the style loss</p> <h3 id="style-loss-1">Style Loss</h3> <p>For style extraction, gram matrices are used same as in images. Gram Matrix \(G \epsilon R^{ NxN }\), where \(G_{ij}\) is the inner product between the feature maps \(i\) and \(j\) represented by vectors and \(N\) is the number feature maps. The difference here is that the feature maps here are 1- Dimensional whereas in images they are 2D. Also, as we are using a model with only one layer, there is no notation of \(l\). Let \(F_{ij}\) be the spectrogram i.e. the encoding of the audio of \(i^{th}\) filter at \(j^{th}\) position.<br> \(G_{ij}= \sum_{k}F_{ik}F_{jk}\).<br> Style loss is the Mean squared error between the gram matrices of Style audio and the generated audio i.e. content audio.<br> Let \(\vec{a}\) be the style audio and \(\vec{x}\) be the generated audio. Let \(A\) and \(X\) be the style representations of of style audio and generated audio with \(N\) number of channels(or number of filters) and \(M\) number of samples. So, Total style loss is \(L(\vec{a},\vec{x})_{style}\).<br> \(L(\vec{a},\vec{x})_{style} = \frac{\sum_{i,j}(X_{ij}-A_{ij})^{2}}{4N^{2}M^{2}}\).<br> Here, only one layer is present so there’s no significance of weighting of layer.</p> <h3 id="post-processing">Post-processing</h3> <p>After generation of audio phase reconstruction is done so as to convert the audio back to time domain from frequency domain. Griffin-Lim algorithm is used for reconstruction. Also, instead of using white noise to generate the final audio, the content audio is used which also prevents calculations as content loss is no longer needed, only style loss is used which is similar to images.</p> <h3 id="hyperparameter-tuning-1">Hyperparameter tuning</h3> <p>To calculate the style loss, standard error back-propagation is done.</p> <h1 id="future-work"><ins>Future Work</ins></h1> <p>With the coming of new generative models like Generative Adversarial Networks the neural style transfer algorithm can be modified and can be used for better results.</p> <h1 id="technology-overview"><ins>Technology Overview</ins></h1> <p>Python 2.7 is used for implementation and the model is implemented using Deep Learning library PyTorch. Librosa is used for audio analysis. The model is executed on Intel® AI DevCloud which is 3x to 4x faster than the workstation being used, Intel® AI DevCloud runs the models and processes on high-performance and efficient Intel® Xeon® processors.</p> <h1 id="references"><ins>References</ins></h1> <h3 id="papers-">Papers :</h3> <ol> <li>Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. “Image style transfer using convolutional neural networks.” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.</li> <li>Grinstein, Eric, et al. “Audio style transfer.” arXiv preprint arXiv:1710.11385 (2017).</li> </ol> <h3 id="blogs-">Blogs :</h3> <ol> <li><a href="https://dmitryulyanov.github.io/audio-texture-synthesis-and-style-transfer/" rel="external nofollow noopener" target="_blank">Audio texture synthesis and style transfer by Dmitry Ulyanov and Vadim Lebedev</a></li> </ol> <h3 id="code-implementations-">Code Implementations :</h3> <ol> <li><a href="http://pytorch.org/tutorials/advanced/neural_style_tutorial.html#sphx-glr-advanced-neural-style-tutorial-py" rel="external nofollow noopener" target="_blank">Advanced Pytorch Tutorial for Neural Style Transfer</a></li> </ol> <h3 id="technical-components-">Technical Components :</h3> <ol> <li><a href="https://librosa.github.io/librosa/" rel="external nofollow noopener" target="_blank">Librosa</a></li> <li><a href="https://pytorch.org/" rel="external nofollow noopener" target="_blank">Pytorch</a></li> <li><a href="https://software.intel.com/en-us/ai-academy/devcloud" rel="external nofollow noopener" target="_blank">Intel AI DevCloud</a></li> </ol> <h3 id="project-resources-">Project Resources :</h3> <ol> <li><a href="https://devmesh.intel.com/projects/neural-style-transfer-on-audio-signals" rel="external nofollow noopener" target="_blank">Intel DevMesh Project</a></li> <li><a href="https://github.com/alishdipani/Neural-Style-Transfer-Audio" rel="external nofollow noopener" target="_blank">Github Repository</a></li> </ol> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/gsoc2019/2019/08/22/Wrapping-up-GSoC-2019/">Wrapping up GSoC 2019</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/gsoc2019/2019/08/22/IRuby-integration-and-ticks/">IRuby integration and ticks</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/gsoc2019/2019/07/26/The-show-and-the-plot-functions/">The show and the plot functions</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/gsoc2019/2019/07/13/Multi-plots-in-Rubyplot/">Multi plots in Rubyplot</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/gsoc2019/2019/06/28/Simple-Plots-in-Rubyplot/">Simple Plots in Rubyplot</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Alish Dipani. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-home",title:"Home",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-journals",title:"Journals",description:"",section:"Navigation",handler:()=>{window.location.href="/journals/"}},{id:"nav-conferences",title:"Conferences",description:"",section:"Navigation",handler:()=>{window.location.href="/conferences/"}},{id:"nav-teaching",title:"Teaching",description:"",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"nav-awards",title:"Awards",description:"",section:"Navigation",handler:()=>{window.location.href="/awards/"}},{id:"nav-service",title:"Service",description:"",section:"Navigation",handler:()=>{window.location.href="/service/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"dropdown-invited-talks",title:"Invited Talks",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-experience",title:"Experience",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-open-source",title:"Open-source",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"post-wrapping-up-gsoc-2019",title:"Wrapping up GSoC 2019",description:"",section:"Posts",handler:()=>{window.location.href="/gsoc2019/2019/08/22/Wrapping-up-GSoC-2019/"}},{id:"post-iruby-integration-and-ticks",title:"IRuby integration and ticks",description:"",section:"Posts",handler:()=>{window.location.href="/gsoc2019/2019/08/22/IRuby-integration-and-ticks/"}},{id:"post-the-show-and-the-plot-functions",title:"The show and the plot functions",description:"",section:"Posts",handler:()=>{window.location.href="/gsoc2019/2019/07/26/The-show-and-the-plot-functions/"}},{id:"post-multi-plots-in-rubyplot",title:"Multi plots in Rubyplot",description:"",section:"Posts",handler:()=>{window.location.href="/gsoc2019/2019/07/13/Multi-plots-in-Rubyplot/"}},{id:"post-simple-plots-in-rubyplot",title:"Simple Plots in Rubyplot",description:"",section:"Posts",handler:()=>{window.location.href="/gsoc2019/2019/06/28/Simple-Plots-in-Rubyplot/"}},{id:"post-the-scatter-plot-example",title:"The Scatter plot example",description:"",section:"Posts",handler:()=>{window.location.href="/gsoc2019/2019/06/10/The-Scatter-plot-example/"}},{id:"post-rubyplot-installation-guide",title:"Rubyplot installation guide",description:"",section:"Posts",handler:()=>{window.location.href="/gsoc2019/2019/06/09/Rubyplot-installation-guide/"}},{id:"post-gsoc-2019-project-introduction",title:"GSoC 2019 project introduction",description:"",section:"Posts",handler:()=>{window.location.href="/gsoc2019/2019/06/08/GSoC-2019-project-introduction/"}},{id:"post-neural-style-transfer-on-audio-signals",title:"Neural Style Transfer on Audio Signals",description:"",section:"Posts",handler:()=>{window.location.href="/projects/2018/08/Neural-Style-Transfer-Audio/"}},{id:"news-i-will-be-a-lead-ta-for-the-neuroai-https-neuroai-neuromatch-io-tutorials-intro-html-neuromatch-academy",title:"I will be a Lead TA for the [NeuroAI](https://neuroai.neuromatch.io/tutorials/intro.html) Neuromatch Academy",description:"",section:"News"},{id:"news-i-created-a-tutorial-on-normalization-https-neuroai-neuromatch-io-tutorials-w1d5-microcircuits-student-w1d5-tutorial2-html-for-the-neuroai-https-neuroai-neuromatch-io-tutorials-intro-html-neuromatch-academy",title:"I created a tutorial on [Normalization](https://neuroai.neuromatch.io/tutorials/W1D5_Microcircuits/student/W1D5_Tutorial2.html) for the [NeuroAI](https://neuroai.neuromatch.io/tutorials/intro.html) Neuromatch Academy",description:"",section:"News"},{id:"news-our-commentary-paper-quot-linking-faces-to-social-cognition-the-temporal-pole-as-a-potential-social-switch-https-www-pnas-org-doi-abs-10-1073-pnas-2411735121-quot-has-been-published-in-pnas",title:"Our commentary paper &quot;[Linking faces to social cognition: The temporal pole as a...",description:"",section:"News"},{id:"news-i-will-be-giving-a-talk-at-sinha-lab-department-of-brain-amp-cognitive-sciences-mit-usa",title:"I will be giving a talk at Sinha Lab, Department of Brain &amp;...",description:"",section:"News"},{id:"news-i-will-be-joining-the-psychology-phd-program-at-georgiatech-as-a-coco-fellow-https-coco-psych-gatech-edu",title:"I will be joining the Psychology PhD program at GeorgiaTech as a [CoCo...",description:"",section:"News"},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%61%6C%69%73%68.%64%69%70%61%6E%69@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=i028n20AAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/alishdipani","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/alishdipani","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>